# Necessary packages

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import numpy as np
import pandas as pd

from data_gen import  get_train_test_data

# from data_generator import data_generator
from utils import supervised_model_training
from pategan import pategan
from sklearn.preprocessing import MinMaxScaler


def reconstruct_and_save_synthetic_data(synth_data, label_encoders, scaler, expected_columns, output_file="synthetic_data.csv"):
    """
    Reconstruct synthetic data from its normalized and encoded form.
    
    Args:
        synth_data (np.ndarray): Synthetic data generated by the model.
        label_encoders (dict): Label encoders for categorical columns.
        scaler (MinMaxScaler): Scaler used for normalization.
        expected_columns (list): Expected column names for the reconstructed data.
        output_file (str): Path to save the reconstructed CSV file.
    
    Returns:
        None
    """
    # Inverse scale the numerical columns
    reconstructed_data = scaler.inverse_transform(synth_data)

    # Check if the number of columns matches
    if len(expected_columns) != reconstructed_data.shape[1]:
        print(f"Warning: Column mismatch! Adjusting column names.")
        expected_columns = [f"feature_{i}" for i in range(reconstructed_data.shape[1])]

    # Create DataFrame
    reconstructed_df = pd.DataFrame(reconstructed_data, columns=expected_columns)

    # Revert categorical columns using label encoders
    for column, encoder in label_encoders.items():
        if encoder is not None and column in reconstructed_df:
            reconstructed_df[column] = encoder.inverse_transform(reconstructed_df[column].round().astype(int))

    # Remove decimals from numerical columns (convert float columns to int)
    for column in reconstructed_df.columns:
        if reconstructed_df[column].dtype == float:
            # Round and convert to int
            reconstructed_df[column] = reconstructed_df[column].apply(lambda x: int(round(x)))

    # Ensure no column retains decimal points
    reconstructed_df = reconstructed_df.applymap(lambda x: int(x) if isinstance(x, float) else x)

    # Save the reconstructed data to a CSV file
    reconstructed_df.to_csv(output_file, index=False)
    print(f"Synthetic data has been reconstructed and saved to {output_file}")



def pategan_main(args):
    """PATEGAN Main function."""
    models = ['logisticregression', 'randomforest', 'gaussiannb', 'bernoullinb',
              'svmlin', 'Extra Trees', 'LDA', 'AdaBoost', 'Bagging', 'gbm', 'xgb']

    # Load and split the data
    train_data, test_data, label_encoders, scaler ,col= get_train_test_data()
    data_dim = train_data.shape[1]

    results = np.zeros([len(models), 4])

    parameters = {'n_s': args.n_s, 'batch_size': args.batch_size, 'k': args.k,
                  'epsilon': args.epsilon, 'delta': args.delta,
                  'lamda': args.lamda}

    best_perf = 0.0

    # Generate synthetic training data
    for it in range(args.iterations):
        print('Iteration', it)
        synth_train_data_temp = pategan(train_data, parameters)
        temp_perf, _ = supervised_model_training(
            synth_train_data_temp[:, :(data_dim - 1)],
            np.round(synth_train_data_temp[:, (data_dim - 1)]),
            train_data[:, :(data_dim - 1)],
            np.round(train_data[:, (data_dim - 1)]),
            'logisticregression')

        if temp_perf > best_perf:
            best_perf = temp_perf.copy()
            synth_train_data = synth_train_data_temp.copy()

        print('Iteration:', it + 1)
        print('Best-Perf:', best_perf)

    # Train supervised models
    for model_index in range(len(models)):
        model_name = models[model_index]

        # Using original data
        results[model_index, 0], results[model_index, 2] = (
            supervised_model_training(train_data[:, :(data_dim - 1)],
                                      np.round(train_data[:, (data_dim - 1)]),
                                      test_data[:, :(data_dim - 1)],
                                      np.round(test_data[:, (data_dim - 1)]),
                                      model_name))

        # Using synthetic data
        results[model_index, 1], results[model_index, 3] = (
            supervised_model_training(synth_train_data[:, :(data_dim - 1)],
                                      np.round(synth_train_data[:, (data_dim - 1)]),
                                      test_data[:, :(data_dim - 1)],
                                      np.round(test_data[:, (data_dim - 1)]),
                                      model_name))

    results = pd.DataFrame(np.round(results, 4),
                           columns=['AUC-Original', 'AUC-Synthetic',
                                    'APR-Original', 'APR-Synthetic'])
    print(results)
    print('Averages:')
    print(results.mean(axis=0))

    print(train_data, "\n")
    print("output:\n")
    print(synth_train_data)

    # Extract original column names
    original_columns = label_encoders.keys()
    # Debugging prints for checking the column mismatch
    print(f"Synthetic data shape: {synth_train_data.shape}")
    print(f"Original columns: {original_columns}")

    # Ensure the number of columns match before reconstruction
    # if len(original_columns) != synth_train_data.shape[1]:
    #     print(f"Column mismatch! Expected {len(original_columns)} columns, but got {synth_train_data.shape[1]} columns.")
    # else:
    #     # Proceed with reconstruction if columns match
        # reconstruct_and_save_synthetic_data(synth_train_data, label_encoders, scaler, original_columns)
    expected_columns = [
      'I-ACCOUNTNUM', 'I-BUILDINGNUM', 'I-CITY', 'I-CREDITCARDNUMBER', 'I-DATEOFBIRTH', 
      'I-DRIVERLICENSENUM', 'I-EMAIL', 'I-GIVENNAME', 'I-IDCARDNUM', 'I-PASSWORD', 
      'I-SOCIALNUM', 'I-STREET', 'I-SURNAME', 'I-TAXNUM', 'I-TELEPHONENUM', 
      'I-USERNAME', 'I-ZIPCODE'
    ]

    reconstruct_and_save_synthetic_data(synth_train_data, label_encoders, scaler, expected_columns)


    return results, train_data, synth_train_data


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_no', help='number of generated data', default=10000, type=int)
    parser.add_argument('--data_dim', help='number of dimensions of generated dimension (if random)', default=10, type=int)
    parser.add_argument('--dataset', help='dataset to use', default='random', type=str)
    parser.add_argument('--noise_rate', help='noise ratio on data', default=1.0, type=float)
    parser.add_argument('--iterations', help='number of iterations for handling initialization randomness', default=50, type=int)
    parser.add_argument('--n_s', help='the number of student training iterations', default=1, type=int)
    parser.add_argument('--batch_size', help='the number of batch size for training student and generator', default=64, type=int)
    parser.add_argument('--k', help='the number of teachers', default=10, type=int)
    parser.add_argument('--epsilon', help='Differential privacy parameters (epsilon)', default=1.0, type=float)
    parser.add_argument('--delta', help='Differential privacy parameters (delta)', default=0.00001, type=float)
    parser.add_argument('--lamda', help='PATE noise size', default=1.0, type=float)

    args = parser.parse_args()

    # Calls main function
    results, ori_data, synth_data = pategan_main(args)
